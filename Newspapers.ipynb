{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23aeea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import duckdb\n",
    "from googleapiclient.discovery import build\n",
    "from google.oauth2 import service_account\n",
    "import time\n",
    "import win32com.client\n",
    "from pretty_html_table import build_table\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6830657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (119.0.6045.105) detected in PATH at C:\\Users\\Shithi.Maitra\\Unilever Codes\\Scraping Scripts\\chromedriver.exe might not be compatible with the detected chrome version (120.0.6099.131); currently, chromedriver 120.0.6099.109 is recommended for chrome 120.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles from: thefinancialexpress.com\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=120.0.6099.131)\nStacktrace:\n\tGetHandleVerifier [0x00007FF6059C82B2+55298]\n\t(No symbol) [0x00007FF605935E02]\n\t(No symbol) [0x00007FF6057F05AB]\n\t(No symbol) [0x00007FF6057D0038]\n\t(No symbol) [0x00007FF605856BC7]\n\t(No symbol) [0x00007FF60586A15F]\n\t(No symbol) [0x00007FF605851E83]\n\t(No symbol) [0x00007FF60582670A]\n\t(No symbol) [0x00007FF605827964]\n\tGetHandleVerifier [0x00007FF605D40AAB+3694587]\n\tGetHandleVerifier [0x00007FF605D9728E+4048862]\n\tGetHandleVerifier [0x00007FF605D8F173+4015811]\n\tGetHandleVerifier [0x00007FF605A647D6+695590]\n\t(No symbol) [0x00007FF605940CE8]\n\t(No symbol) [0x00007FF60593CF34]\n\t(No symbol) [0x00007FF60593D062]\n\t(No symbol) [0x00007FF60592D3A3]\n\tBaseThreadInitThunk [0x00007FFBEC7B7344+20]\n\tRtlUserThreadStart [0x00007FFBEE2426B1+33]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m elem\u001b[38;5;241m.\u001b[39msend_keys(prompts[j] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m consumer goods FMCG news\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# scroll\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m last_height \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_script\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn document.body.scrollHeight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     36\u001b[0m     driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow.scrollTo(0, document.body.scrollHeight);\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:405\u001b[0m, in \u001b[0;36mWebDriver.execute_script\u001b[1;34m(self, script, *args)\u001b[0m\n\u001b[0;32m    402\u001b[0m converted_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(args)\n\u001b[0;32m    403\u001b[0m command \u001b[38;5;241m=\u001b[39m Command\u001b[38;5;241m.\u001b[39mW3C_EXECUTE_SCRIPT\n\u001b[1;32m--> 405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscript\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscript\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverted_args\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:345\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    343\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 345\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    346\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=120.0.6099.131)\nStacktrace:\n\tGetHandleVerifier [0x00007FF6059C82B2+55298]\n\t(No symbol) [0x00007FF605935E02]\n\t(No symbol) [0x00007FF6057F05AB]\n\t(No symbol) [0x00007FF6057D0038]\n\t(No symbol) [0x00007FF605856BC7]\n\t(No symbol) [0x00007FF60586A15F]\n\t(No symbol) [0x00007FF605851E83]\n\t(No symbol) [0x00007FF60582670A]\n\t(No symbol) [0x00007FF605827964]\n\tGetHandleVerifier [0x00007FF605D40AAB+3694587]\n\tGetHandleVerifier [0x00007FF605D9728E+4048862]\n\tGetHandleVerifier [0x00007FF605D8F173+4015811]\n\tGetHandleVerifier [0x00007FF605A647D6+695590]\n\t(No symbol) [0x00007FF605940CE8]\n\t(No symbol) [0x00007FF60593CF34]\n\t(No symbol) [0x00007FF60593D062]\n\t(No symbol) [0x00007FF60592D3A3]\n\tBaseThreadInitThunk [0x00007FFBEC7B7344+20]\n\tRtlUserThreadStart [0x00007FFBEE2426B1+33]\n"
     ]
    }
   ],
   "source": [
    "## scrape\n",
    "\n",
    "# newspapers\n",
    "prompts = [\"The Financial Express\", \"The Daily Star\", \"The Business Standard\", \"Prothom Alo\"]\n",
    "sources = [\"thefinancialexpress.com\", \"thedailystar.net\", \"tbsnews.net\", \"prothomalo.com\"]\n",
    "\n",
    "# accumulators\n",
    "start_time = time.time()\n",
    "df_acc = pd.DataFrame()\n",
    "timings = []\n",
    "\n",
    "# preference\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"ignore-certificate-errors\")\n",
    "\n",
    "# open window\n",
    "driver = webdriver.Chrome(service=Service(), options=options)\n",
    "driver.maximize_window()\n",
    "\n",
    "# iterate\n",
    "source_count = len(sources)\n",
    "for j in range(0, source_count):\n",
    "    \n",
    "    # link\n",
    "    link = \"https://www.google.com/\"\n",
    "    driver.get(link)\n",
    "\n",
    "    # search\n",
    "    print(\"Fetching articles from: \" + sources[j])\n",
    "    elem = driver.find_element(By.CLASS_NAME, \"gLFyf\")\n",
    "    elem.send_keys(prompts[j] + \" consumer goods FMCG news\\n\")\n",
    "\n",
    "    # scroll\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while(1):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height: break\n",
    "        last_height = new_height\n",
    "\n",
    "    # soup\n",
    "    soup_init = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    soup = soup_init.find_all(\"div\", attrs={\"class\": \"N54PNb BToiNc cvP2Ce\"})\n",
    "\n",
    "    # scrape\n",
    "    headline = []\n",
    "    publish_date = []\n",
    "    excerpt = []\n",
    "    path = []\n",
    "    url = []\n",
    "    pos_in_search = []\n",
    "    report_date = []\n",
    "    news_count = len(soup)\n",
    "    for i in range(0, news_count):\n",
    "\n",
    "        # headline\n",
    "        try: val = soup[i].find(\"h3\", attrs={\"class\": \"LC20lb MBeuO DKV0Md\"}).get_text()\n",
    "        except: val = None\n",
    "        headline.append(val)\n",
    "\n",
    "        # publication date\n",
    "        try: val = soup[i].find(\"span\", attrs={\"class\": \"lhLbod gEBHYd\"}).get_text()\n",
    "        except: val = None\n",
    "        publish_date.append(val)\n",
    "\n",
    "        # excerpt\n",
    "        val = soup[i].find(\"div\", attrs={\"class\": \"kb0PBd cvP2Ce\"}).get_text()\n",
    "        val = val.split(publish_date[i])[1] if publish_date[i] is not None else val\n",
    "        excerpt.append(val)\n",
    "\n",
    "        # path\n",
    "        try: val = soup[i].find(\"cite\", attrs={\"class\": \"qLRx3b tjvcx GvPZzd cHaqb\"}).get_text()\n",
    "        except: val = None\n",
    "        path.append(val)\n",
    "        \n",
    "        # url\n",
    "        try: val = soup[i].find(\"a\", attrs={\"jsname\": \"UWckNb\"})[\"href\"]\n",
    "        except: val = None\n",
    "        url.append(val)\n",
    "\n",
    "        # position\n",
    "        pos_in_search.append(i + 1)\n",
    "        \n",
    "        # timing\n",
    "        timing = str(time.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        report_date.append(timing)\n",
    "\n",
    "    # accumulate \n",
    "    df = pd.DataFrame()\n",
    "    df['headline'] = headline\n",
    "    df['publish_date'] = [p[0:-3] if p is not None else p for p in publish_date]\n",
    "    df['excerpt'] = excerpt\n",
    "    df['path'] = path\n",
    "    df['url'] = url\n",
    "    df['position'] = pos_in_search\n",
    "    df['newspaper'] = sources[j]\n",
    "    df['report_date'] = report_date\n",
    "    df = duckdb.query('''select * from df where path like '%''' + sources[j] +  '''%' ''').df()\n",
    "    df_acc = df_acc.append(df, ignore_index=True)\n",
    "    timings.append(timing)\n",
    "    \n",
    "# close window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23f1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GSheet\n",
    "\n",
    "# credentials\n",
    "SERVICE_ACCOUNT_FILE = \"read-write-to-gsheet-apis-1-04f16c652b1e.json\"\n",
    "SAMPLE_SPREADSHEET_ID = \"1gkLRp59RyRw4UFds0-nNQhhWOaS4VFxtJ_Hgwg2x2A0\"\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\"]\n",
    "\n",
    "# APIs\n",
    "creds = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "service = build(\"sheets\", \"v4\", credentials=creds)\n",
    "sheet = service.spreadsheets()\n",
    "\n",
    "# extract\n",
    "values = sheet.values().get(spreadsheetId=SAMPLE_SPREADSHEET_ID, range='News!A1:I').execute().get('values', [])\n",
    "df_acc_prev = pd.DataFrame(values[1:], columns = values[0])\n",
    "\n",
    "# transform\n",
    "qry = '''\n",
    "-- old\n",
    "select headline, publish_date, excerpt, path, url, position, newspaper, 0 if_new, report_date\n",
    "from df_acc_prev\n",
    "union all\n",
    "-- new\n",
    "select \n",
    "    headline, publish_date, excerpt, path, url, position, newspaper, \n",
    "    case \n",
    "        when publish_date like '%23' then 1\n",
    "        when publish_date like '%২৩' then 1\n",
    "        when publish_date like '%ago' then 1\n",
    "        when publish_date like '%আগে' then 1\n",
    "        else 0\n",
    "    end if_new, \n",
    "    report_date\n",
    "from df_acc\n",
    "where url not in(select url from df_acc_prev)\n",
    "'''\n",
    "df_acc_pres = duckdb.query(qry).df()\n",
    "\n",
    "# load\n",
    "sheet.values().clear(spreadsheetId=SAMPLE_SPREADSHEET_ID, range='News').execute()\n",
    "sheet.values().update(spreadsheetId=SAMPLE_SPREADSHEET_ID, range='News!A1', valueInputOption='USER_ENTERED', body={'values': [df_acc_pres.columns.values.tolist()] + df_acc_pres.fillna('').values.tolist()}).execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a224421",
   "metadata": {},
   "outputs": [],
   "source": [
    "## novelty\n",
    "\n",
    "# new articles\n",
    "df_acc_new = duckdb.query('''select * from df_acc_pres where if_new=1''').df()\n",
    "new_heads = df_acc_new['headline'].tolist()\n",
    "new_links = df_acc_new['url'].tolist()\n",
    "new_dates = df_acc_new['publish_date'].tolist()\n",
    "new_len = df_acc_new.shape[0]\n",
    "\n",
    "# latest pull\n",
    "timing_df = pd.DataFrame()\n",
    "timing_df['source'] = sources\n",
    "timing_df['timing'] = timings\n",
    "\n",
    "# store\n",
    "if new_len > 0:\n",
    "    with pd.ExcelWriter(\"C:/Users/Shithi.Maitra/Downloads/newspaper_fmcg_scrapings.xlsx\") as writer:\n",
    "        df_acc_pres.to_excel(writer, sheet_name=\"All Results\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a307625",
   "metadata": {},
   "outputs": [],
   "source": [
    "## email\n",
    "\n",
    "# email\n",
    "ol = win32com.client.Dispatch(\"outlook.application\")\n",
    "olmailitem = 0x0\n",
    "newmail = ol.CreateItem(olmailitem)\n",
    "\n",
    "# summary\n",
    "qry = '''\n",
    "select newspaper, total_articles, new_articles, last_report_time, latest_report_time\n",
    "from \n",
    "    (select newspaper, count(url) total_articles, count(case when if_new=1 then url else null end) new_articles\n",
    "    from df_acc_pres\n",
    "    group by 1\n",
    "    ) tbl1 \n",
    "\n",
    "    inner join\n",
    "\n",
    "    (select newspaper, max(report_date) last_report_time\n",
    "    from df_acc_prev\n",
    "    group by 1\n",
    "    ) tbl2 using(newspaper)\n",
    "    \n",
    "    inner join\n",
    "\n",
    "    (select source newspaper, timing latest_report_time\n",
    "    from timing_df\n",
    "    ) tbl3 using(newspaper)\n",
    "'''\n",
    "summ_df = duckdb.query(qry).df()\n",
    "\n",
    "# subject, recipients\n",
    "newmail.Subject = 'Newspaper Scrapings - FMCG'\n",
    "# newmail.To = 'shithi.maitra@unilever.com'\n",
    "newmail.To = 'zoya.rashid@unilever.com'\n",
    "newmail.CC = 'mehedi.asif@unilever.com; samsuddoha.nayeem@unilever.com; sudipta.saha@unilever.com; asif.rezwan@unilever.com'\n",
    "\n",
    "# body\n",
    "newmail.HTMLbody = f'''\n",
    "Dear concern,<br><br>\n",
    "Please find recent FMCG articles from the popular English dailies attached. New articles are reported on <a href=\"https://teams.microsoft.com/l/channel/19%3ae8f5c9a9e7374b51840112d6280374af%40thread.tacv2/FMCG%2520News?groupId=1b8eee70-e11c-419e-966f-d830a968c87a&tenantId=f66fae02-5d36-495b-bfe0-78a6ff9f8e6e\">Teams FMCG News</a>. Here are statistics from the latest pull: \n",
    "''' + build_table(summ_df, random.choice(['green_light', 'red_light', 'blue_light', 'grey_light', 'orange_light']), font_size='12px', text_align='left') + '''\n",
    "More newspapers, online news portals or even Bangla dailies can be incorporated on demand. This is an auto-generated email via <i>win32com</i>.<br><br>\n",
    "Thanks,<br>\n",
    "Shithi Maitra<br>\n",
    "Asst. Manager, CSE<br>\n",
    "Unilever BD Ltd.<br>\n",
    "'''\n",
    "\n",
    "# attachment(s) \n",
    "folder = \"C:/Users/Shithi.Maitra/Downloads/\"\n",
    "filename = folder + \"newspaper_fmcg_scrapings.xlsx\"\n",
    "newmail.Attachments.Add(filename)\n",
    "\n",
    "# send\n",
    "if new_len > 0: newmail.Send()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a1fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MSTeams\n",
    "\n",
    "# email\n",
    "ol = win32com.client.Dispatch(\"outlook.application\")\n",
    "olmailitem = 0x0\n",
    "newmail = ol.CreateItem(olmailitem)\n",
    "\n",
    "# report\n",
    "new = \"⚠ The following \" + str(new_len) + \" article(s) are newly found, as of \" + timing\n",
    "for i in range(0, new_len): new = new + '''<br>&nbsp;&nbsp;&nbsp;• <a href=\"''' + new_links[i] + '''\">''' + new_heads[i] + '''</a> [''' + new_dates[i] + ''']''' \n",
    "    \n",
    "# Teams\n",
    "newmail.Subject = \"New FMCG Articles!\"\n",
    "newmail.To = \"FMCG News - Auto Monitoring <062c1c6b.Unilever.onmicrosoft.com@emea.teams.ms>\"\n",
    "newmail.HTMLbody = new + \"<br><br>\"\n",
    "if new_len > 0: newmail.Send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b98923",
   "metadata": {},
   "outputs": [],
   "source": [
    "## stats\n",
    "display(df_acc_pres.head())\n",
    "print(\"Articles in result: \" + str(df_acc_pres.shape[0]))\n",
    "print(\"Elapsed time to report (mins): \" + str(round((time.time() - start_time) / 60.00, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea641e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
