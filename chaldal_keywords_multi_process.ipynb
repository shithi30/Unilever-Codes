{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa7b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import multiprocessing\n",
    "from tabulate import tabulate\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import time\n",
    "import win32com.client\n",
    "from pretty_html_table import build_table\n",
    "# just in case\n",
    "import base64\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c0e920",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chaldal ##\n",
    "def scrape_chaldal_process(keywords): \n",
    "    \n",
    "    # accumulators\n",
    "    brands = ['Boost', 'Clear Shampoo', 'Clear Men Shampoo', 'Clear Hijab', 'Simple', 'Pepsodent', 'Brylcreem', 'Bru', 'St. Ives', 'Horlicks', 'Sunsilk', 'Lux', 'Ponds', \"Pond's\", 'Closeup', 'Cif', 'Dove', 'Maltova', 'Domex', 'Clinic', 'Tresemme', 'Tresemm√©', 'GlucoMax', 'Knorr', 'Glow & Lovely', 'Glow & Handsome', 'Wheel', 'Axe', 'Pureit', 'Lifebuoy', 'Surf Excel', 'Vaseline', 'Vim', 'Rin']\n",
    "    df_acc_local = pd.DataFrame()\n",
    "    lock = multiprocessing.Lock()\n",
    "\n",
    "    # open window\n",
    "    driver = webdriver.Chrome('chromedriver', options=[])\n",
    "    driver.maximize_window()\n",
    "\n",
    "    for k in keywords:\n",
    "        # url\n",
    "        url = \"https://chaldal.com/search/\" + k\n",
    "        driver.get(url)\n",
    "        \n",
    "        # scroll\n",
    "        SCROLL_PAUSE_TIME = 5\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height: break\n",
    "            last_height = new_height\n",
    "\n",
    "        # soup\n",
    "        soup_init = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        soup = soup_init.find_all(\"div\", attrs={\"class\": \"imageWrapper\"})\n",
    "        \n",
    "        # scrape\n",
    "        skus = []\n",
    "        quants = []\n",
    "        prices = []\n",
    "        prices_if_discounted = []\n",
    "        for s in soup:\n",
    "            # sku\n",
    "            try: val = s.find(\"div\", attrs={\"class\": \"name\"}).get_text()\n",
    "            except: val = None\n",
    "            skus.append(val)\n",
    "            # quantity\n",
    "            try: val = s.find(\"div\", attrs={\"class\": \"subText\"}).get_text()\n",
    "            except: val = None\n",
    "            quants.append(val)\n",
    "            # price\n",
    "            try: val = float(s.find(\"div\", attrs={\"class\": \"price\"}).get_text().split()[1].replace(',', ''))\n",
    "            except: val = None\n",
    "            prices.append(val)\n",
    "            # discount\n",
    "            try: val = float(s.find(\"div\", attrs={\"class\": \"discountedPrice\"}).get_text().split()[1].replace(',', ''))\n",
    "            except: val = None\n",
    "            prices_if_discounted.append(val)\n",
    "        \n",
    "        # accumulate\n",
    "        df = pd.DataFrame()\n",
    "        df['sku'] = skus\n",
    "        df['keyword'] = k\n",
    "        df['quantity'] = quants\n",
    "        df['price'] = prices\n",
    "        df['price_if_discounted'] = prices_if_discounted\n",
    "        \n",
    "        # relevant data\n",
    "        qry = '''\n",
    "        select *\n",
    "        from\n",
    "            (select *, row_number() over() pos_in_pg\n",
    "            from df\n",
    "            ) tbl1 \n",
    "        where replace(sku, ' ', '') ilike ''' + \"'%\" + k.replace(\" \", \"\") + '''%';\n",
    "        '''\n",
    "        df = duckdb.query(qry).df()\n",
    "        rel_idx = df['pos_in_pg'].tolist()\n",
    "        len_rel_idx = len(rel_idx)\n",
    "        \n",
    "        # description\n",
    "        bnr = 3\n",
    "        try: driver.find_element(By.CLASS_NAME, \"important-banner\")\n",
    "        except: bnr = 2\n",
    "        descs = []\n",
    "        report_times = []\n",
    "        for i in range(0, len_rel_idx): \n",
    "            report_times.append(time.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            descs.append(\"ERROR\")\n",
    "            try:\n",
    "                # move\n",
    "                path = '//*[@id=\"page\"]/div/div[6]/section/div/div/div/div/section/div['+str(bnr)+']/div[2]/div['+str(rel_idx[i])+']/div/div'\n",
    "                elem = driver.find_element(By.XPATH, path)\n",
    "                mov = ActionChains(driver).move_to_element(elem)\n",
    "                mov.perform()\n",
    "                # details\n",
    "                path1 = '//*[@id=\"page\"]/div/div[6]/section/div/div/div/div/section/div['+str(bnr)+']/div[2]/div['+str(rel_idx[i])+']/div/div/div[5]/span/a'\n",
    "                path2 = '//*[@id=\"page\"]/div/div[6]/section/div/div/div/div/section/div['+str(bnr)+']/div[2]/div['+str(rel_idx[i])+']/div/div/div[6]/span/a'\n",
    "                try: elem = driver.find_element(By.XPATH, path1)\n",
    "                except: elem = driver.find_element(By.XPATH, path2)\n",
    "                elem.click()\n",
    "                # content\n",
    "                path = '//*[@id=\"page\"]/div/div[6]/section/div/div/div/div/section/div['+str(bnr)+']/div[2]/div['+str(rel_idx[i])+']/div/div[2]/div/div/article/section[2]/div[5]'\n",
    "                elem = driver.find_element(By.XPATH, path)\n",
    "                descs[i] = elem.text.replace(\"\\n\", \" \")\n",
    "                # close\n",
    "                path = '//*[@id=\"page\"]/div/div[6]/section/div/div/div/div/section/div['+str(bnr)+']/div[2]/div['+str(rel_idx[i])+']/div/div[2]/div/button'\n",
    "                elem = driver.find_element(By.XPATH, path)\n",
    "                elem.click()\n",
    "            except: pass\n",
    "            \n",
    "        # conglomerate\n",
    "        if_ubls = []\n",
    "        skus = [skus[i-1] for i in rel_idx]\n",
    "        for i in range(0, len_rel_idx):\n",
    "            if_ubls.append(None)\n",
    "            for b in brands:\n",
    "                if b.lower() + ' ' in skus[i].lower():\n",
    "                    if_ubls[i] = True\n",
    "                    break     \n",
    "        \n",
    "        # accumulate\n",
    "        df['if_unilever'] = if_ubls\n",
    "        df['description'] = descs\n",
    "        df['report_time'] = report_times\n",
    "        df_acc_local = df_acc_local.append(df)\n",
    "        \n",
    "        # progress\n",
    "        lock.acquire()\n",
    "        print(\"Data fetched for keyword: \" + k)\n",
    "        lock.release()\n",
    "        \n",
    "    # close window\n",
    "    driver.close()\n",
    "    \n",
    "    # return\n",
    "    return df_acc_local\n",
    "    \n",
    "def scrape_chaldal(folder):\n",
    "    \n",
    "    # accumulators\n",
    "    start_time = time.time()\n",
    "    df_acc = pd.DataFrame()\n",
    "    keywords = ['conditioner', 'handwash', 'bodywash', 'facewash', 'lotion', 'cream', 'toothpaste', 'dishwash', 'toilet clean', 'soup', 'shampoo', 'health drink', 'detergent', 'moisturizer', 'soap', 'petroleum jelly', 'hair oil', 'germ kill']\n",
    "    process_count = 3\n",
    "    keywords_chunks = [keywords[i::process_count] for i in range(process_count)]\n",
    "    \n",
    "    # processes\n",
    "    pool = multiprocessing.Pool(process_count)\n",
    "    dfs_acc = pool.map(scrape_chaldal_process, keywords_chunks)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    # csv\n",
    "    for i in range(0, process_count): df_acc = df_acc.append(dfs_acc[i])\n",
    "    filename = folder + r\"\\chaldal_unilever_keywords_data.csv\"    \n",
    "    df_acc.to_csv(filename, index=False)           \n",
    "    \n",
    "    # stats\n",
    "    print(\"\\nTotal SKUs found: \" + str(df_acc.shape[0]))\n",
    "    elapsed_time = str(round((time.time() - start_time) / 60.00, 2))\n",
    "    print(\"Elapsed time to run processes (mins): \" + elapsed_time)\n",
    "    \n",
    "    # return\n",
    "    return df_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80633f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_viz(scraped_df, folder):\n",
    "    \n",
    "    # summary\n",
    "    qry = '''\n",
    "    select\n",
    "        'Chaldal' platform,\n",
    "        count(sku) \"SKUs\", \n",
    "        count(case when if_unilever=true then sku else null end) \"UBL SKUs\", \n",
    "        count(case when if_unilever=true then sku else null end)*1.00/count(sku) \"SoS\",\n",
    "        count(case when if_unilever=true and pos_in_pg<11 then sku else null end)*1.00/(count(distinct keyword)*10) \"top-10 SoS\", \n",
    "        count(case when if_unilever=true and pos_in_pg<11 and length(description)=0 then sku else null end) \"top-10 SoS missing description\", \n",
    "        count(case when if_unilever=true and length(description)=0 then sku else null end) \"UBL SKUs missing description\", \n",
    "        count(case when description='ERROR' then sku else null end) \"description errors\"\n",
    "    from scraped_df; \n",
    "    '''\n",
    "    anls_df = duckdb.query(qry).df()\n",
    "    print(tabulate(anls_df, headers='keys', tablefmt='psql', showindex=False))\n",
    "    \n",
    "    # email\n",
    "    ol = win32com.client.Dispatch(\"outlook.application\")\n",
    "    olmailitem = 0x0\n",
    "    newmail = ol.CreateItem(olmailitem)\n",
    "\n",
    "    # subject, recipients\n",
    "    newmail.Subject = 'Chaldal Keyword SoS & Wordcloud'\n",
    "    newmail.To = 'shithi.maitra@unilever.com'\n",
    "    # newmail.To = 'mehedi.asif@unilever.com'\n",
    "    # newmail.CC = 'mehedi.asif@unilever.com; zakeea.husain@unilever.com; rakaanjum.unilever@gmail.com; nazmussajid.ubl@gmail.com'\n",
    "    \n",
    "    # UBL cloud\n",
    "    qry = ''' select description from scraped_df where if_unilever=true; '''\n",
    "    df = duckdb.query(qry).df()\n",
    "    text = \" \".join(d for d in df['description'].fillna(\"\").tolist())\n",
    "    word_cloud = WordCloud(width=300, height=200, random_state=1, background_color=\"white\", colormap=\"ocean\", collocations=False, stopwords=STOPWORDS).generate(text)\n",
    "    word_cloud.to_file(folder + r\"\\ubl_cloud.png\")\n",
    "    cloud1_html = f'<img src=\"cid: MyId1\" style=\"border: 1px solid; padding: 5px; background-color: white; display: block\" width=\"96%\" height=\"100%\"><figcaption><b>Fig-01:</b> UBL wordcloud</figcaption>'\n",
    "\n",
    "    # non UBL cloud\n",
    "    qry = ''' select description from scraped_df where if_unilever is null; '''\n",
    "    df = duckdb.query(qry).df()\n",
    "    text = \" \".join(d for d in df['description'].fillna(\"\").tolist())\n",
    "    word_cloud = WordCloud(width=300, height=200, random_state=1, background_color=\"white\", colormap=\"Set2\", collocations=False, stopwords=STOPWORDS).generate(text)\n",
    "    word_cloud.to_file(folder + r\"\\nonubl_cloud.png\")\n",
    "    cloud2_html = f'<img src=\"cid: MyId2\" style=\"border: 1px solid; padding: 5px; background-color: white; display: block\" width=\"96%\" height=\"100%\"><figcaption><b>Fig-02:</b> non-UBL wordcloud</figcaption>'\n",
    "    \n",
    "    # body\n",
    "    newmail.HTMLbody = f'''\n",
    "    Dear concern,<br><br>\n",
    "    Today's <i>Chaldal</i> UBL share of search (SoS) and descriptions for selected keywords have been scraped. A brief statistics of the process is given below:\n",
    "    ''' + build_table(anls_df, 'blue_light') + '''\n",
    "    <table style=\"margin-left: auto; margin-right: auto\">\n",
    "        <tr>\n",
    "            <td>''' + cloud1_html + '''</td>\n",
    "            <td>''' + cloud2_html + '''</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    <br>\n",
    "    Two wordclouds based on UBL and non-UBL descriptions are presented above. Please find the full dataset attached. Note, this email was auto generated using <i>win32com</i>.<br><br>\n",
    "    Thanks,<br>\n",
    "    Shithi Maitra<br>\n",
    "    Asst. Manager, Cust. Service Excellence<br>\n",
    "    Unilever BD Ltd.<br>\n",
    "    '''\n",
    "    \n",
    "    # attachment\n",
    "    filename = folder + r\"\\chaldal_unilever_keywords_data.csv\"\n",
    "    newmail.Attachments.Add(filename)\n",
    "    newmail.Attachments.Add(folder + r\"\\ubl_cloud.png\").PropertyAccessor.SetProperty(\"http://schemas.microsoft.com/mapi/proptag/0x3712001F\", \"MyId1\")\n",
    "    newmail.Attachments.Add(folder + r\"\\nonubl_cloud.png\").PropertyAccessor.SetProperty(\"http://schemas.microsoft.com/mapi/proptag/0x3712001F\", \"MyId2\")\n",
    "    \n",
    "    # send\n",
    "    newmail.Send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e18675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# call\n",
    "if __name__ == \"__main__\":\n",
    "    folder = r\"C:\\Users\\Shithi.Maitra\\Unilever Codes\\Scraping Scripts\\Chaldal Stocks\"\n",
    "    scraped_df = scrape_chaldal(folder)\n",
    "    send_viz(scraped_df, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00e069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "# \"C:\\Users\\Shithi.Maitra\\Downloads\\chaldal_keywords_multi_process.py\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
