{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "209566b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped from page: 1, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 2, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 3, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 4, SKUs found: 40, waiting time (sec): 4\n",
      "Data scraped from page: 5, SKUs found: 40, waiting time (sec): 4\n",
      "Data scraped from page: 6, SKUs found: 40, waiting time (sec): 4\n",
      "Data scraped from page: 7, SKUs found: 40, waiting time (sec): 5\n",
      "Data scraped from page: 8, SKUs found: 40, waiting time (sec): 5\n",
      "Data scraped from page: 9, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 10, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 11, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 12, SKUs found: 40, waiting time (sec): 4\n",
      "Data scraped from page: 13, SKUs found: 40, waiting time (sec): 5\n",
      "Data scraped from page: 14, SKUs found: 40, waiting time (sec): 5\n",
      "Data scraped from page: 15, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 16, SKUs found: 40, waiting time (sec): 4\n",
      "Data scraped from page: 17, SKUs found: 40, waiting time (sec): 4\n",
      "Data scraped from page: 18, SKUs found: 40, waiting time (sec): 5\n",
      "Data scraped from page: 19, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 20, SKUs found: 40, waiting time (sec): 5\n",
      "Data scraped from page: 21, SKUs found: 40, waiting time (sec): 5\n",
      "Data scraped from page: 22, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 23, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 24, SKUs found: 40, waiting time (sec): 4\n",
      "Data scraped from page: 25, SKUs found: 40, waiting time (sec): 4\n",
      "Data scraped from page: 26, SKUs found: 40, waiting time (sec): 5\n",
      "Data scraped from page: 27, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 28, SKUs found: 40, waiting time (sec): 4\n",
      "Data scraped from page: 29, SKUs found: 40, waiting time (sec): 4\n",
      "Data scraped from page: 30, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 31, SKUs found: 40, waiting time (sec): 4\n",
      "Data scraped from page: 32, SKUs found: 40, waiting time (sec): 5\n",
      "Data scraped from page: 33, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 34, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 35, SKUs found: 40, waiting time (sec): 4\n",
      "Data scraped from page: 36, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 37, SKUs found: 40, waiting time (sec): 5\n",
      "Data scraped from page: 38, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 39, SKUs found: 40, waiting time (sec): 5\n",
      "Data scraped from page: 40, SKUs found: 40, waiting time (sec): 5\n",
      "Data scraped from page: 41, SKUs found: 40, waiting time (sec): 5\n",
      "Data scraped from page: 42, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 43, SKUs found: 40, waiting time (sec): 5\n",
      "Data scraped from page: 44, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 45, SKUs found: 40, waiting time (sec): 5\n",
      "Data scraped from page: 46, SKUs found: 40, waiting time (sec): 5\n",
      "Data scraped from page: 47, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 48, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 49, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 50, SKUs found: 40, waiting time (sec): 5\n",
      "Data scraped from page: 51, SKUs found: 40, waiting time (sec): 3\n",
      "Data scraped from page: 52, SKUs found: 40, waiting time (sec): 4\n",
      "Data scraped from page: 53, SKUs found: 40, waiting time (sec): 5\n",
      "Data scraped from page: 54, SKUs found: 40, waiting time (sec): 5\n",
      "Data scraped from page: 55, SKUs found: 40, waiting time (sec): 4\n",
      "Data scraped from page: 56, SKUs found: 21, waiting time (sec): 5\n",
      "\n",
      "Total SKUs found: 2221\n",
      "Elapsed time to scrape (mins): 6.27\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "from sympy.parsing.sympy_parser import parse_expr\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "import random\n",
    "import duckdb\n",
    "import win32com.client\n",
    "from pretty_html_table import build_table\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# accumulators\n",
    "start_time = time.time()\n",
    "df_acc = pd.DataFrame()\n",
    "pg = 1\n",
    "\n",
    "while (1):\n",
    "    # soup\n",
    "    link = 'https://www.daraz.com.bd/catalog/?from=filter&location=-21&page=' + str(pg) + '&q=unilever'\n",
    "    html = requests.get(link, verify=False).text\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    # list of key-pair vals\n",
    "    try:\n",
    "        list_of_dic_str = str(soup)\n",
    "        list_of_dic_str = list_of_dic_str.split('\"listItems\":', 1)[1].split(',\"breadcrumb\"', 1)[0]\n",
    "        list_of_dic_str = list_of_dic_str.replace(\"true\", \"True\")\n",
    "        list_of_dic_str = list_of_dic_str.replace(\"false\", \"False\")\n",
    "        list_of_dic = parse_expr(list_of_dic_str, evaluate=False)\n",
    "    except:\n",
    "        break\n",
    "\n",
    "    # SKUs' data\n",
    "    title = []\n",
    "    price = []\n",
    "    original_price = []\n",
    "    rating = []\n",
    "    review = []\n",
    "    brand = []\n",
    "\n",
    "    l = len(list_of_dic)\n",
    "    for i in range(0, l):\n",
    "        title.append(list_of_dic[i]['name'])\n",
    "        brand.append(list_of_dic[i]['name'].split()[0].replace(\"'\", \"\"))\n",
    "        price.append(float(list_of_dic[i]['price']))\n",
    "        review.append(int(list_of_dic[i]['review']))\n",
    "        try:\n",
    "            original_price.append(float(list_of_dic[i]['originalPrice']))\n",
    "        except:\n",
    "            original_price.append(float(list_of_dic[i]['price']))\n",
    "        try:\n",
    "            rating.append(float(list_of_dic[i]['ratingScore']))\n",
    "        except:\n",
    "            rating.append(None)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['title'] = title\n",
    "    df['brand_derived'] = brand\n",
    "    df['discounted_price'] = price\n",
    "    df['original_price'] = original_price\n",
    "    df['rating'] = rating\n",
    "    df['review'] = review\n",
    "    df['source_page'] = pg\n",
    "    df_acc = df_acc.append(df)\n",
    "\n",
    "    wt = random.randint(3, 5)\n",
    "    print(\"Data scraped from page: \" + str(pg) + \", SKUs found: \" + str(df.shape[0]) + \", waiting time (sec): \" + str(wt))\n",
    "    time.sleep(wt)\n",
    "\n",
    "    pg = pg + 1\n",
    "\n",
    "# stats\n",
    "print()\n",
    "print(\"Total SKUs found: \" + str(df_acc.shape[0]))\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time to scrape (mins): \" + str(round(elapsed_time / 60.00, 2)))\n",
    "df_acc = df_acc.reset_index(drop=True)\n",
    "\n",
    "# csv\n",
    "qry = '''\n",
    "select \n",
    "    title, \n",
    "    brand_derived, \n",
    "    original_price, \n",
    "    case \n",
    "        when (discounted_price<original_price) then discounted_price\n",
    "        else null\n",
    "    end discounted_price, \n",
    "    case \n",
    "        when (discounted_price<original_price) then (original_price-discounted_price)/original_price\n",
    "        else null\n",
    "    end discount_pct, \n",
    "    rating, \n",
    "    review, \n",
    "    source_page\n",
    "from df_acc; \n",
    "'''\n",
    "df_acc = duckdb.query(qry).df()\n",
    "df_acc.to_csv(\"daraz_unilever_skus_data.csv\", index=False)\n",
    "\n",
    "# analysis\n",
    "qry = '''\n",
    "select \n",
    "    brand_derived brand, \n",
    "    count(*) \"SKUs enlisted\", \n",
    "    count(case when discounted_price is not null then title else null end) \"SKUs given discount\",\n",
    "    avg(discount_pct) \"avg. discount pct\",\n",
    "    avg(rating) \"avg. rating\", \n",
    "    sum(review)::int \"reviews recorded\"\n",
    "from \n",
    "    df_acc tbl1 \n",
    "\n",
    "    inner join \n",
    "\n",
    "    (select brand_derived, count(*) skus\n",
    "    from df_acc \n",
    "    where source_page=1\n",
    "    group by 1 \n",
    "    order by 2 desc\n",
    "    limit 7\n",
    "    ) tbl2 using(brand_derived)\n",
    "group by 1\n",
    "order by 2 desc; \n",
    "'''\n",
    "res_df = duckdb.query(qry).df()\n",
    "\n",
    "# # email\n",
    "# ol = win32com.client.Dispatch(\"outlook.application\")\n",
    "# olmailitem = 0x0\n",
    "# newmail = ol.CreateItem(olmailitem)\n",
    "#\n",
    "# # subject, recipients\n",
    "# newmail.Subject = 'Scraped & Analysed: daraz.com.bd'\n",
    "# newmail.To = 'avra.barua@unilever.com'\n",
    "# newmail.CC = 'mehedi.asif@unilever.com'\n",
    "#\n",
    "# # body\n",
    "# newmail.HTMLbody = f'''\n",
    "# Hello Bhaiya,<br><br>\n",
    "# Please find below an analysis of popular Unilever SKUs available on daraz.com.bd (filtered for Bangladesh).<br>\n",
    "# ''' + build_table(res_df, 'blue_light') + '''\n",
    "# Also, the complete search results are available in the attachment for your convenience.<br><br>\n",
    "# Note that, the data was extracted at ''' + time.strftime('%d-%b-%y, %I:%M%p') + '''. This is an auto generated email using smtplib.<br><br>\n",
    "# Thanks,<br>\n",
    "# Shithi Maitra<br>\n",
    "# Asst. Manager, Cust. Service Excellence<br>\n",
    "# Unilever BD Ltd.<br>\n",
    "# '''\n",
    "#\n",
    "# # attachment(s)\n",
    "# attachment = ['daraz_unilever_skus_data.csv']\n",
    "# for atch in attachment:\n",
    "#   newmail.Attachments.Add(os.getcwd() + '\\\\' + atch)\n",
    "#\n",
    "# # display, send\n",
    "# # newmail.Display()\n",
    "# newmail.Send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbf711f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
